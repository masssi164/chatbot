# LiteLLM Proxy Configuration for Ollama Integration
# OpenAI-compatible API gateway to Ollama models

model_list:
  # Chat models via ollama_chat (recommended by LiteLLM)
  - model_name: "llama3.2:1b"
    litellm_params:
      model: "ollama_chat/llama3.2:1b"
      api_base: http://ollama:11434
      keep_alive: "8m"  # Keep model in RAM for 8 minutes
      
  - model_name: "phi3.5:3.8b"
    litellm_params:
      model: "ollama_chat/phi3.5:3.8b"
      api_base: http://ollama:11434
      keep_alive: "8m"
      
  - model_name: "qwen2.5:1.5b"
    litellm_params:
      model: "ollama_chat/qwen2.5:1.5b"
      api_base: http://ollama:11434
      keep_alive: "8m"

# LiteLLM settings
litellm_settings:
  drop_params: True          # Drop unsupported OpenAI params
  set_verbose: True          # Enable verbose logging
  
# Router settings for load balancing
router_settings:
  routing_strategy: "least-busy"  # Route to least busy model
  
# General proxy settings
general_settings:
  # Share master key with backend OPENAI_API_KEY to avoid LiteLLM DB lookup errors
  master_key: ${OPENAI_API_KEY:-sk-local-master}
  alerting: []
  
# Environment variables
environment_variables:
  OLLAMA_HOST: "0.0.0.0:11434"
