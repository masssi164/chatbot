services:
  n8n:
    image: docker.n8n.io/n8nio/n8n:latest
    restart: unless-stopped
    ports:
      - "${N8N_PORT:-5678}:5678"
    env_file:
      - .env
    environment:
      N8N_HOST: ${N8N_HOST}
      N8N_PORT: 5678
      N8N_PROTOCOL: ${N8N_PROTOCOL}
      WEBHOOK_URL: ${N8N_WEBHOOK_URL}
      N8N_EDITOR_BASE_URL: ${N8N_EDITOR_BASE_URL}
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: postgres-n8n
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_DATABASE: ${POSTGRES_DB:-n8n}
      DB_POSTGRESDB_USER: ${POSTGRES_USER:-n8n}
      DB_POSTGRESDB_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      GENERIC_TIMEZONE: ${GENERIC_TIMEZONE}
      TZ: ${GENERIC_TIMEZONE}
      N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS: "true"
      N8N_RUNNERS_ENABLED: "true"
    volumes:
      - n8n_data:/home/node/.n8n
    depends_on:
      postgres-n8n:
        condition: service_healthy

  # PostgreSQL for n8n
  postgres-n8n:
    image: postgres:16-alpine
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-n8n}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      POSTGRES_DB: ${POSTGRES_DB:-n8n}
    volumes:
      - postgres_n8n_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-n8n} -d ${POSTGRES_DB:-n8n} > /dev/null 2>&1"]
      interval: 5s
      timeout: 5s
      retries: 10

  # PostgreSQL for chatbot-backend  
  postgres-chatbot:
    image: postgres:16-alpine
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${DB_USER:-chatbot_user}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-chatbot_password}
      POSTGRES_DB: ${DB_NAME:-chatbot_db}
    volumes:
      - postgres_chatbot_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-chatbot_user} -d ${DB_NAME:-chatbot_db} > /dev/null 2>&1"]
      interval: 5s
      timeout: 5s
      retries: 10

  chatbot-backend:
    build:
      context: ./chatbot-backend
      dockerfile: Dockerfile
    image: app.chatbot/chatbot-backend:latest
    restart: unless-stopped
    ports:
      - "${BACKEND_PORT:-8080}:8080"
    environment:
      SPRING_PROFILES_ACTIVE: ${SPRING_PROFILES_ACTIVE:-prod}
      # Database configuration (production)
      DB_HOST: postgres-chatbot
      DB_PORT: 5432
      DB_NAME: ${DB_NAME:-chatbot_db}
      DB_USER: ${DB_USER:-chatbot_user}
      DB_PASSWORD: ${DB_PASSWORD:-chatbot_password}
      # OpenAI configuration
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-https://api.openai.com/v1}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      # MCP configuration
      MCP_ENCRYPTION_KEY: ${MCP_ENCRYPTION_KEY:-prod-32-character-secret-key-here-change-me}
      # JVM settings
      JAVA_OPTS: "-Xmx1024m -Xms512m"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8080/actuator/health || exit 1"]
      interval: 30s
      timeout: 3s
      start_period: 60s
      retries: 5
    depends_on:
      postgres-chatbot:
        condition: service_healthy

  chatbot-frontend:
    build:
      context: ./chatbot
      dockerfile: Dockerfile
    image: app.chatbot/chatbot-frontend:latest
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT:-3000}:80"
    environment:
      # Vite environment variables for API URL
      VITE_API_BASE_URL: ${VITE_API_BASE_URL:-http://localhost:8080}
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost/ || exit 1"]
      interval: 30s
      timeout: 3s
      start_period: 5s
      retries: 3
    depends_on:
      chatbot-backend:
        condition: service_healthy

  # Ollama - Local LLM Server
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      # Uncomment for browser CORS access (optional)
      # - OLLAMA_ORIGINS=*
    volumes:
      - ollama_models:/root/.ollama
    # Uncomment for NVIDIA GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "ollama list > /dev/null 2>&1 || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 20
      start_period: 60s

  # Ollama Seeder - Preload required models before LiteLLM starts
  ollama-seeder:
    build:
      context: ./docker/ollama-seeder
    restart: "no"
    environment:
      OLLAMA_HOST: ${OLLAMA_HOST:-ollama}
      OLLAMA_PORT: ${OLLAMA_PORT:-11434}
      OLLAMA_API_BASE: ${OLLAMA_API_BASE:-http://ollama:11434}
      OLLAMA_SEED_MODELS: ${OLLAMA_SEED_MODELS:-llama3.2,mistral,qwen2.5:7b}
      OLLAMA_SEED_TIMEOUT: ${OLLAMA_SEED_TIMEOUT:-600}
    depends_on:
      ollama:
        condition: service_healthy

  # LiteLLM - OpenAI-compatible API Gateway
  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    restart: unless-stopped
    ports:
      - "${LITELLM_PORT:-4000}:4000"
    environment:
      # Admin UI credentials (optional but recommended)
      - UI_USERNAME=${LITELLM_UI_USERNAME:-admin}
      - UI_PASSWORD=${LITELLM_UI_PASSWORD:-admin}
      # Master key for API access
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-local-master}
      # Logging
      - JSON_LOGS=True
      - OLLAMA_API_BASE=http://ollama:11434
    volumes:
      - ./config/litellm.config.yaml:/app/config.yaml:ro
    command: ["--config", "/app/config.yaml", "--detailed_debug"]
    depends_on:
      ollama:
        condition: service_healthy
      ollama-seeder:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:4000/health"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 30s

volumes:
  n8n_data:
  postgres_n8n_data:
  postgres_chatbot_data:
  ollama_models:
