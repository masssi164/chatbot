services:
  n8n:
    image: docker.n8n.io/n8nio/n8n:latest
    restart: unless-stopped
    ports:
      - "${N8N_PORT:-5678}:5678"
    env_file:
      - .env
    environment:
      N8N_HOST: ${N8N_HOST}
      N8N_PORT: 5678
      N8N_PROTOCOL: ${N8N_PROTOCOL}
      WEBHOOK_URL: ${N8N_WEBHOOK_URL}
      N8N_EDITOR_BASE_URL: ${N8N_EDITOR_BASE_URL}
      N8N_SECURE_COOKIE: ${N8N_SECURE_COOKIE:-false}
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: postgres-n8n
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_DATABASE: ${POSTGRES_DB:-n8n}
      DB_POSTGRESDB_USER: ${POSTGRES_USER:-n8n}
      DB_POSTGRESDB_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      GENERIC_TIMEZONE: ${GENERIC_TIMEZONE}
      TZ: ${GENERIC_TIMEZONE}
      N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS: "true"
      N8N_RUNNERS_ENABLED: "true"
    volumes:
      - n8n_data:/home/node/.n8n
    depends_on:
      postgres-n8n:
        condition: service_healthy

  # PostgreSQL for n8n
  postgres-n8n:
    image: postgres:16-alpine
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-n8n}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      POSTGRES_DB: ${POSTGRES_DB:-n8n}
    volumes:
      - postgres_n8n_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-n8n} -d ${POSTGRES_DB:-n8n} > /dev/null 2>&1"]
      interval: 5s
      timeout: 5s
      retries: 10

  # PostgreSQL for chatbot-backend  
  postgres-chatbot:
    image: postgres:16-alpine
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${DB_USER:-chatbot_user}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-chatbot_password}
      POSTGRES_DB: ${DB_NAME:-chatbot_db}
    volumes:
      - postgres_chatbot_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-chatbot_user} -d ${DB_NAME:-chatbot_db} > /dev/null 2>&1"]
      interval: 5s
      timeout: 5s
      retries: 10

  chatbot-backend:
    build:
      context: ./chatbot-backend
      dockerfile: Dockerfile
    image: app.chatbot/chatbot-backend:latest
    restart: unless-stopped
    ports:
      - "${BACKEND_PORT:-8080}:8080"
    environment:
      SPRING_PROFILES_ACTIVE: ${SPRING_PROFILES_ACTIVE:-prod}
      # Database configuration (production)
      DB_HOST: postgres-chatbot
      DB_PORT: 5432
      DB_NAME: ${DB_NAME:-chatbot_db}
      DB_USER: ${DB_USER:-chatbot_user}
      DB_PASSWORD: ${DB_PASSWORD:-chatbot_password}
      # OpenAI configuration
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-http://localagi:8080/v1}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-sk-local-master}
      # MCP configuration
      MCP_ENCRYPTION_KEY: ${MCP_ENCRYPTION_KEY:-prod-32-character-secret-key-here-change-me}
      # JVM settings
      JAVA_OPTS: "-Xmx1024m -Xms512m"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8080/actuator/health || exit 1"]
      interval: 30s
      timeout: 3s
      start_period: 60s
      retries: 5
    depends_on:
      postgres-chatbot:
        condition: service_healthy
      localagi:
        condition: service_healthy

  chatbot-frontend:
    build:
      context: ./chatbot
      dockerfile: Dockerfile
    image: app.chatbot/chatbot-frontend:latest
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT:-3000}:80"
    environment:
      # Vite environment variables for API URL
      VITE_API_BASE_URL: ${VITE_API_BASE_URL:-http://localhost:8080}
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost/ || exit 1"]
      interval: 10s
      timeout: 10s
      start_period: 60s
      retries: 10
    depends_on:
      chatbot-backend:
        condition: service_healthy

  # LocalAI - deterministic local model runtime
  localai:
    image: localai/localai:v3.7.0-aio-cpu
    restart: unless-stopped
    ports:
      - "${LOCALAI_PORT:-8082}:8080"
    environment:
      - API_KEY=${LOCALAI_API_KEY:-localai-demo-key}
      - THREADS=${LOCALAI_THREADS:-4}
      - MODELS_PATH=/root/.localai/models
    volumes:
      - localai_models:/root/.localai/models
    healthcheck:
      test: ["CMD-SHELL","curl -fsS http://127.0.0.1:8080/readyz || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 20s

  # LocalAI Seeder - install GGUF models into shared volume
  localai-seeder:
    image: localai/localai:v3.7.0-aio-cpu
    restart: "no"
    entrypoint: ["/bin/sh","/scripts/seeder.sh"]
    environment:
      - LOCALAI_SEED_MODELS=${LOCALAI_SEED_MODELS:-huggingface://TheBloke/Llama-3.2-1B-Instruct-GGUF/llama-3.2-1b-instruct-q4_k_m.gguf}
    volumes:
      - ./docker/localai-seeder/seeder.sh:/scripts/seeder.sh:ro
      - localai_models:/root/.localai/models
    depends_on:
      localai:
        condition: service_healthy

  # LocalAGI - OpenAI Responses-compatible agent gateway
  localagi:
    image: ghcr.io/mudler/localagi:v0.1.0
    restart: unless-stopped
    ports:
      - "${LOCALAGI_PORT:-8083}:8080"
    environment:
      - API_KEY=${OPENAI_API_KEY:-sk-local-master}
      - LOCALAI_BACKEND=http://localai:8080
      - ENABLE_MCP=true
    depends_on:
      localai:
        condition: service_healthy
      localai-seeder:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL","curl -fsS http://127.0.0.1:8080/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

volumes:
  n8n_data:
  postgres_n8n_data:
  postgres_chatbot_data:
  localai_models:
